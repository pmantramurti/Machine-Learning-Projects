{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install english-words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km4yNSQa7JDR",
        "outputId": "be42a475-ef69-4f8c-c662-95015f26566f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting english-words\n",
            "  Downloading english-words-1.1.0.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: english-words\n",
            "  Building wheel for english-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for english-words: filename=english_words-1.1.0-py3-none-any.whl size=1106680 sha256=26df0712a930154c17294caab88aec3203c894761fbd2fbbe2807fc6eb4c8e47\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/3d/4c/12a119ce90b46b4f90f9ddf41d719ecabb40faec6103379fc8\n",
            "Successfully built english-words\n",
            "Installing collected packages: english-words\n",
            "Successfully installed english-words-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import wordnet as wn\n",
        "import nltk\n",
        "from english_words import english_words_lower_alpha_set\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.book import text4\n",
        "import math"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPjLS_3qxKUq",
        "outputId": "5c90f74d-f588-40f9-df67-eacc20041e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1 : Wordnet\n",
        "Wordnet is a lexical database of the English language, which groups words by part of speech and concept they represent. On the surface level, its similar to a Thesaurus, but Wordnet establishes the semantical relationships between words, while a Thesaurus simply groups words by similar meanings."
      ],
      "metadata": {
        "id": "ZPuSq68evlHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2 : Synsets"
      ],
      "metadata": {
        "id": "2RSszNTqw2Yb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obkuGfckfFWS",
        "outputId": "a01d2ce9-e1d9-4ef8-ce27-df82237ba5ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('chair.n.01'),\n",
              " Synset('professorship.n.01'),\n",
              " Synset('president.n.04'),\n",
              " Synset('electric_chair.n.01'),\n",
              " Synset('chair.n.05'),\n",
              " Synset('chair.v.01'),\n",
              " Synset('moderate.v.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "noun = \"chair\"\n",
        "wn.synsets(noun)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q3 : Extract Synset"
      ],
      "metadata": {
        "id": "2SUTQizyx7RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Definition : \" + wn.synset('chair.n.01').definition())\n",
        "print(\"Examples : \" + str(wn.synset('chair.n.01').examples()))\n",
        "print(\"Lemmas : \" + str(wn.synset('chair.n.01').lemmas()))\n",
        "print(\"Traversing Hierarchy\")\n",
        "curr_set = wn.synset('chair.n.01').hypernyms()\n",
        "while(len(curr_set) > 0):\n",
        "  print(curr_set)\n",
        "  curr_set = curr_set[0].hypernyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC2MCMAsx9mW",
        "outputId": "ddafc578-d6e9-4c54-c34f-94674cfe6733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition : a seat for one person, with a support for the back\n",
            "Examples : ['he put his coat over the back of the chair and sat down']\n",
            "Lemmas : [Lemma('chair.n.01.chair')]\n",
            "Traversing Hierarchy\n",
            "[Synset('seat.n.03')]\n",
            "[Synset('furniture.n.01')]\n",
            "[Synset('furnishing.n.02')]\n",
            "[Synset('instrumentality.n.03')]\n",
            "[Synset('artifact.n.01')]\n",
            "[Synset('whole.n.02')]\n",
            "[Synset('object.n.01')]\n",
            "[Synset('physical_entity.n.01')]\n",
            "[Synset('entity.n.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hierarchy of wordnet is organized into specific objetc, then more and more general terms, until it reaches the point to entity, which is the definition of a noun."
      ],
      "metadata": {
        "id": "h2HHUtkT2OvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q4 : The 'Nyms"
      ],
      "metadata": {
        "id": "5hxeXhEK2dO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hypernyms : \" + str(wn.synset('chair.n.01').hypernyms()))\n",
        "print(\"Hyponyms : \" + str(wn.synset('chair.n.01').hyponyms()))\n",
        "print(\"Meronyms : \" + str(wn.synset('chair.n.01').part_meronyms()))\n",
        "print(\"Holonyms : \" + str(wn.synset('chair.n.01').part_holonyms()))\n",
        "print(\"Antonyms : \" + str(wn.synset('chair.n.01').lemmas()[0].antonyms()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJP6hXmv3FOg",
        "outputId": "ef66080b-15b6-4da9-b7d6-285423efe382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypernyms : [Synset('seat.n.03')]\n",
            "Hyponyms : [Synset('armchair.n.01'), Synset('barber_chair.n.01'), Synset('chair_of_state.n.01'), Synset('chaise_longue.n.01'), Synset('eames_chair.n.01'), Synset('fighting_chair.n.01'), Synset('folding_chair.n.01'), Synset('highchair.n.01'), Synset('ladder-back.n.01'), Synset('lawn_chair.n.01'), Synset('rocking_chair.n.01'), Synset('straight_chair.n.01'), Synset('swivel_chair.n.01'), Synset('tablet-armed_chair.n.01'), Synset('wheelchair.n.01')]\n",
            "Meronyms : [Synset('back.n.08'), Synset('leg.n.03')]\n",
            "Holonyms : []\n",
            "Antonyms : []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q5 : Verb Synsets"
      ],
      "metadata": {
        "id": "zJ0FUOku4Yj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verb = \"run\"\n",
        "verb_synsets = wn.synsets(verb)\n",
        "print(verb_synsets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVDkPfuh4cx0",
        "outputId": "f3e265b5-1ec9-41a8-da66-e7dddbde7fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('run.n.01'), Synset('test.n.05'), Synset('footrace.n.01'), Synset('streak.n.01'), Synset('run.n.05'), Synset('run.n.06'), Synset('run.n.07'), Synset('run.n.08'), Synset('run.n.09'), Synset('run.n.10'), Synset('rivulet.n.01'), Synset('political_campaign.n.01'), Synset('run.n.13'), Synset('discharge.n.06'), Synset('run.n.15'), Synset('run.n.16'), Synset('run.v.01'), Synset('scat.v.01'), Synset('run.v.03'), Synset('operate.v.01'), Synset('run.v.05'), Synset('run.v.06'), Synset('function.v.01'), Synset('range.v.01'), Synset('campaign.v.01'), Synset('play.v.18'), Synset('run.v.11'), Synset('tend.v.01'), Synset('run.v.13'), Synset('run.v.14'), Synset('run.v.15'), Synset('run.v.16'), Synset('prevail.v.03'), Synset('run.v.18'), Synset('run.v.19'), Synset('carry.v.15'), Synset('run.v.21'), Synset('guide.v.05'), Synset('run.v.23'), Synset('run.v.24'), Synset('run.v.25'), Synset('run.v.26'), Synset('run.v.27'), Synset('run.v.28'), Synset('run.v.29'), Synset('run.v.30'), Synset('run.v.31'), Synset('run.v.32'), Synset('run.v.33'), Synset('run.v.34'), Synset('ply.v.03'), Synset('hunt.v.01'), Synset('race.v.02'), Synset('move.v.13'), Synset('melt.v.01'), Synset('ladder.v.01'), Synset('run.v.41')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q6 : Select Verb Synset"
      ],
      "metadata": {
        "id": "71IWezyM4uTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verb_syn = 'run.v.03'\n",
        "print(\"Definition : \" + wn.synset(verb_syn).definition())\n",
        "print(\"Examples : \" + str(wn.synset(verb_syn).examples()))\n",
        "print(\"Lemmas : \" + str(wn.synset(verb_syn).lemmas()))\n",
        "print(\"Traversing Hierarchy\")\n",
        "curr_set = wn.synset(verb_syn).hypernyms()\n",
        "while(len(curr_set) > 0):\n",
        "  print(str(curr_set[0]) + \" = \" + str(curr_set[0].definition()))\n",
        "  curr_set = curr_set[0].hypernyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUszdQ6s4zkf",
        "outputId": "f976de2c-76c0-4cc1-bb29-364bc4a5d0de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition : stretch out over a distance, space, time, or scope; run or extend between two points or beyond a certain point\n",
            "Examples : ['Service runs all the way to Cranbury', \"His knowledge doesn't go very far\", 'My memory extends back to my fourth year of life', 'The facts extend beyond a consideration of her personal assets']\n",
            "Lemmas : [Lemma('run.v.03.run'), Lemma('run.v.03.go'), Lemma('run.v.03.pass'), Lemma('run.v.03.lead'), Lemma('run.v.03.extend')]\n",
            "Traversing Hierarchy\n",
            "Synset('be.v.03') = occupy a certain position or area; be somewhere\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike Nouns, the verb hierarchy generalizes to \"to be\", in other words defining verbs in general as a state of being. In this case, \"run\", or grammatically, \"running\", is a state of being."
      ],
      "metadata": {
        "id": "VTKf5TIw5ZbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q7 : Morphy"
      ],
      "metadata": {
        "id": "guIjLovW5u5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in english_words_lower_alpha_set:\n",
        "  if wn.morphy(word) == \"run\":\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K589TjnB5yyw",
        "outputId": "e2691508-4c99-4e52-8867-b3f85eeb14b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ran\n",
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q8 : Word Similarity"
      ],
      "metadata": {
        "id": "-bGrJjBM7zmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_1 = wn.synset('stroll.v.01')\n",
        "word_2 = wn.synset('walk.v.01')\n",
        "word_1_leskTest = word_2.definition()\n",
        "word_2_leskTest = word_1.definition()\n",
        "print(\"Wu-Palmer Similarity = \" + str(wn.wup_similarity(word_1, word_2)))\n",
        "lesk_syn_1 = lesk(word_2_leskTest, \"walk\")\n",
        "lesk_syn_2 = lesk(word_1_leskTest, \"stroll\")\n",
        "print(\"Lesk Algorithm word_1 = \" + str(lesk_syn_1) + \" : \" + lesk_syn_1.definition())\n",
        "print(\"Lesk Algorithm word_2 = \" + str(lesk_syn_2) + \" : \" + lesk_syn_2.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9UfI30q72E9",
        "outputId": "bb57a791-1955-4756-ad20-c9ecc4d2abd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wu-Palmer Similarity = 0.8\n",
            "Lesk Algorithm word_1 = Synset('walk.v.10') : take a walk; go for a walk; walk for pleasure\n",
            "Lesk Algorithm word_2 = Synset('amble.n.01') : a leisurely walk (usually in some public place)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main difference is that Wu-Palmer compares the similarity of two words, while Lesk attempts to figure out the correct synset of the word based on teh context of the sentence its in. Because of this, in order to test the word similarity, I used the definition of each word as the sentence of the other word, in order to see if they were similar enough that the word in the other sentence would return the synset, but they did not. To be honest, Lesk Algorithm really isnt suited to this."
      ],
      "metadata": {
        "id": "8r0te7EuA_I1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q9 : SentiWordNet\n",
        "SentiWordNet is basically a lexical analysis method of deducing the emotional charge of a given word, based on the context and the general definitions of said word. One of the best ways to use it is to deduce the tone of a piece of text."
      ],
      "metadata": {
        "id": "wT1sMNYB-O9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"tantrum\"\n",
        "tantrum = swn.senti_synset('tantrum.n.01')\n",
        "print(tantrum)\n",
        "print(\"Positive score = \", tantrum.pos_score())\n",
        "print(\"Negative score = \", tantrum.neg_score())\n",
        "print(\"Objective score = \", tantrum.obj_score())\n",
        "ex_sent = \"The boy threw a tantrum because he wanted more candy\".split(' ')\n",
        "for word in ex_sent:\n",
        "  word1 = swn.senti_synsets(word)\n",
        "  first = True\n",
        "  for item in word1:\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEwT8mqkIx0C",
        "outputId": "a6526e23-5e81-4974-cc9d-fbac51b809cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<fit.n.01: PosScore=0.0 NegScore=0.5>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.5\n",
            "Objective score =  0.5\n",
            "<male_child.n.01: PosScore=0.25 NegScore=0.0>\n",
            "<boy.n.02: PosScore=0.0 NegScore=0.0>\n",
            "<son.n.01: PosScore=0.0 NegScore=0.0>\n",
            "<boy.n.04: PosScore=0.0 NegScore=0.125>\n",
            "<throw.v.01: PosScore=0.0 NegScore=0.0>\n",
            "<throw.v.02: PosScore=0.0 NegScore=0.0>\n",
            "<shed.v.01: PosScore=0.0 NegScore=0.0>\n",
            "<throw.v.04: PosScore=0.0 NegScore=0.0>\n",
            "<give.v.07: PosScore=0.0 NegScore=0.375>\n",
            "<throw.v.06: PosScore=0.0 NegScore=0.0>\n",
            "<project.v.10: PosScore=0.0 NegScore=0.0>\n",
            "<throw.v.08: PosScore=0.0 NegScore=0.0>\n",
            "<bewilder.v.02: PosScore=0.0 NegScore=0.25>\n",
            "<hurl.v.03: PosScore=0.125 NegScore=0.0>\n",
            "<hold.v.03: PosScore=0.125 NegScore=0.0>\n",
            "<throw.v.12: PosScore=0.0 NegScore=0.0>\n",
            "<throw.v.13: PosScore=0.0 NegScore=0.0>\n",
            "<throw.v.14: PosScore=0.0 NegScore=0.0>\n",
            "<confuse.v.02: PosScore=0.125 NegScore=0.625>\n",
            "<angstrom.n.01: PosScore=0.0 NegScore=0.0>\n",
            "<vitamin_a.n.01: PosScore=0.125 NegScore=0.25>\n",
            "<deoxyadenosine_monophosphate.n.01: PosScore=0.0 NegScore=0.0>\n",
            "<adenine.n.01: PosScore=0.0 NegScore=0.0>\n",
            "<ampere.n.02: PosScore=0.0 NegScore=0.0>\n",
            "<a.n.06: PosScore=0.0 NegScore=0.0>\n",
            "<a.n.07: PosScore=0.0 NegScore=0.0>\n",
            "<fit.n.01: PosScore=0.0 NegScore=0.5>\n",
            "<helium.n.01: PosScore=0.0 NegScore=0.0>\n",
            "<he.n.02: PosScore=0.0 NegScore=0.0>\n",
            "<desire.v.01: PosScore=0.25 NegScore=0.0>\n",
            "<want.v.02: PosScore=0.125 NegScore=0.125>\n",
            "<want.v.03: PosScore=0.0 NegScore=0.0>\n",
            "<want.v.04: PosScore=0.0 NegScore=0.0>\n",
            "<want.v.05: PosScore=0.0 NegScore=0.625>\n",
            "<wanted.a.01: PosScore=0.625 NegScore=0.0>\n",
            "<cherished.s.01: PosScore=0.625 NegScore=0.25>\n",
            "<more.n.01: PosScore=0.0 NegScore=0.0>\n",
            "<more.a.01: PosScore=0.0 NegScore=0.0>\n",
            "<more.a.02: PosScore=0.0 NegScore=0.0>\n",
            "<more.r.01: PosScore=0.0 NegScore=0.0>\n",
            "<more.r.02: PosScore=0.0 NegScore=0.0>\n",
            "<candy.n.01: PosScore=0.0 NegScore=0.0>\n",
            "<sugarcoat.v.01: PosScore=0.0 NegScore=0.0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What the scores tell the program is basically whether each word has a positive or negative connotation, and the intensity of said word. This can allow the program to gain a second layer of comprehension, by allowing it to not just understand literal meaning, but also the meaning hidden in the tone of the sentence."
      ],
      "metadata": {
        "id": "3i04b8r3UYkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q10 : Collocation\n",
        "Collocations are phrases, in which the words that make them up have more meaning together than they do apart."
      ],
      "metadata": {
        "id": "U7POSaurU2ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text4.collocations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shevlflzWVzA",
        "outputId": "56f83564-a7f5-4d0c-a5fd-2841620eaf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = ' '.join(text4.tokens)\n",
        "collo = \"Chief Justice\"\n",
        "vocab = len(set(text))\n",
        "hg = text.count('Chief Justice')/vocab\n",
        "print(\"p(Chief Justice) = \",hg )\n",
        "h = text.count('Chief')/vocab\n",
        "print(\"p(Chief) = \", h)\n",
        "g = text.count('Justice')/vocab\n",
        "print('p(Justice) = ', g)\n",
        "pmi = math.log2(hg / (h * g))\n",
        "print('Mutual Information = ', pmi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK1ZDtHIYaJe",
        "outputId": "f14115b1-26be-4ad8-b2d6-53b1da78fd72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p(Chief Justice) =  0.16666666666666666\n",
            "p(Chief) =  0.3333333333333333\n",
            "p(Justice) =  0.2857142857142857\n",
            "Mutual Information =  0.8073549220576041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on a Mutual Information that comes to about 0.81, \"Chief Justice\" is very likely to be a collocation. In an NLTK application, this would be helpful because if the program were to fail to catch collocations, it would lose the comprehension that comes from the added meaning from the combination fo the words."
      ],
      "metadata": {
        "id": "1oxfSbCPaRQX"
      }
    }
  ]
}